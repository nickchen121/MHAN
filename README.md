[TOC]

# Multimodal-enhanced Hierarchical Attention Network (MHAN) for Video Captioning

# English

## Main functions of each file

```
. vector_ Cache (hidden file): Glove vector storage address 
captioning: The address where the text generated by the video description is stored
checkpoints: The address of the saved model file
data: Source video data 
loader: Load data data 
logs: Logs saved during model training, which can be used in conjunction with Tensorboardx
models: Model files 
pycocoevalcap: Code for calculating indicators such as BLEU, CIDER, etc. 
result: The address where the results of the trained model are stored
splits: The method of data cutting 
config.py: Configuration file 
run. py: Test model 
train.py: training model+testing model 
utils. py: Some public methods 
```

## Train

1. First, create a conda virtual environment through cyd.yaml (which stores the project's environment configuration)

`Conda env create - f cyd.yaml`

2. After configuring the environment: directly run the train.py file (if the GPU graphics memory is insufficient, you can reduce the configuration through config)
3. Generally no problem, ask me if you have any questions

## Datasets and other large files

Download link: https://pan.quark.cn/s/44049885ed0b

# 中文（Chinese）

## 各个文件主要功能

```
.vector_cache（隐藏文件）：glove 向量存放地址
captioning：视频描述生成的文本存放的地址
checkpoints：保存的模型文件的地址
data：源视频数据
loader：加载data数据
logs：训练模型时保存的日志，可配合 tensorboardx 一起使用
models：模型文件
pycocoevalcap：BLEU、CIDER…等指标计算的代码
result：训练好的模型的结果存放的地址
splits：数据切割的方式
config.py：配置文件
run.py：测试模型
train.py：训练模型+测试模型
utils.py：一些公共方法
```

## 训练

1. 首先通过 cyd.yaml（里面存放了项目的环境配置） 创建一个 conda 虚拟环境

`conda env create -f cyd.yaml`

2. 配置好环境后：直接运行 train.py 文件（如果GPU 显存不够，可以通过 config 减小配置）
3. 一般没问题，有问题问我

## 数据集和其他大型文件

下载链接：https://pan.quark.cn/s/44049885ed0b



